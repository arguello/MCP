#!/usr/bin/python -u
#
#  sark
#
#   Checkes QueuItems to see if their resources are aviable, if they are, creates a buildjob and assigns resources
#   runs the Jobs through, when job complets calls signalComplete on the attached Commit or Promotion

import os

os.environ.setdefault( "DJANGO_SETTINGS_MODULE", "mcp.settings" )

import sys
import logging
import json
from logging.handlers import SysLogHandler
from datetime import datetime, timedelta
from django.utils.timezone import utc
from django.conf import settings

from mcp.Processor.models import QueueItem, BuildJob, BuildJobNetworkResource
from mcp.Resource.models import Resource
from mcp.lib.Slack import Slack

# in Hours
CLEANUP_DELAY = 0.05
AUTO_ACKNOLEDGE_DELAY = 24.0

PID_FILE = '/var/run/sark.pid'

logging.basicConfig()
logger = logging.getLogger()
handler = SysLogHandler( address='/dev/log', facility=SysLogHandler.LOG_DAEMON )
handler.setFormatter( logging.Formatter( fmt='sark[%(process)d]: %(message)s' ) )
logger.addHandler( handler )
logging.info( 'Starting...' )
if '-v' in sys.argv:
  logger.setLevel( logging.DEBUG )
else:
  logger.setLevel( logging.INFO )

if os.path.exists( PID_FILE ):
  logging.error( 'pid file exists, bailing...' )
  sys.exit( 0 )

tmp = open( PID_FILE, 'w' )
tmp.write( '%s\n' % os.getpid() )
tmp.close()

slack = Slack( 'sark', settings.SITE_NAME, settings.SLACK_PROXY )


# Iterate over the Queued Items
for item in QueueItem.objects.all().order_by( '-priority' ): # start with the biggest number first
  try:
    # Check to see if resources are aviable
    ( compute_status, network_status ) = item.checkResources()
    if compute_status:
      logging.info( 'Queue Item "%s" waiting for compute "%s"' % ( item, compute_status ) )
      item.resource_status = json.dumps( { 'compute': compute_status } )
      item.save()
      continue

    if network_status:
      logging.info( 'Queue Item "%s" waiting for network "%s"' % ( item, network_status ) )
      item.resource_status = json.dumps( { 'network': network_status } )
      item.save()
      continue

  except Exception as e:
    logging.exception( 'Exception "%s" checking resources for item "%s", skiping...' % ( e, item.pk ) )
    item.resource_status = json.dumps( { 'error': 'Exception: "%s"' % e } )
    item.save()
    continue

  job = None
  try:
    # Build a job
    job = BuildJob()
    job.manual = item.manual
    job.build = item.build
    job.project = item.project
    job.branch = item.branch
    job.target = item.target
    job.commit = item.commit
    job.promotion = item.promotion
    job.save()

    # allocate the Resources
    ( compute_resources, network_resources ) = item.allocateResources( job )
    logging.info( 'Starting Queue Item "%s" with Compute "%s", Network "%s", as job "%s"' % ( item, compute_resources, network_resources, job.id ) )
    job.resources = json.dumps( compute_resources )
    job.save()
    for network in network_resources:
      bjnr = BuildJobNetworkResource()
      bjnr.buildjob = job
      bjnr.networkresource = network_resources[ network ]
      bjnr.name = network
      bjnr.save()

  except Exception as e:
    logging.exception( 'Exception "%s" creating job for queue item "%s", skiping...' % ( e, item.pk ) )
    if job and job.pk:
      for bjnr in job.buildjobnetworkresource_set.all():
        bjnr.delete()
      job.delete()

    item.resource_status = json.dumps( { 'error': 'Exception: "%s"' % e } )
    item.save()
    continue

  # remove the queue Item
  slack.post_message( 'Job %s ( project: "%s", build: "%s", branch: "%s", target: "%s" ) for Queue Item %s submitted.' % ( job.id, job.project.name, job.build.name, job.branch, job.target, item.id ), slack.INFO )
  item.delete()


# Iterate over the build new jobs
for job in BuildJob.objects.filter( built_at__isnull=True, ran_at__isnull=True, reported_at__isnull=True, released_at__isnull=True ):
  ready = True
  resource_map = json.loads( job.resources )
  for name in resource_map:
    for index in range( 0, len( resource_map[ name ] ) ):
      if resource_map[ name ][ index ][ 'status' ] not in ( 'Allocated', 'Building' ):
        continue # don't want to overwrite any new status that might of been put in by something else, hopfully that something else waited till it was built, b/c we are going to assume it is, really should keep track of feedback status and building status seperately

      tmp = Resource.built( resource_map[ name ][ index ][ 'config' ] )
      ready &= tmp
      job.updateResourceState( name, index, 'Built' if tmp else 'Building' )

  # all done, set to built
  if ready:
    logging.info( 'Setting job "%s" to Built.' % job.id )
    job.built_at = datetime.utcnow().replace( tzinfo=utc )
    job.save()


# the Job will flag it's self as Ran


# Iterate over the Ran jobs, and signalComplete
for job in BuildJob.objects.filter( built_at__isnull=False, ran_at__isnull=False, reported_at__isnull=True, released_at__isnull=True ):
  if job.commit is not None:
    job.commit.signalComplete( job.target, job.build.name, json.loads( job.resources ) )

  elif job.promotion is not None:
    job.promotion.signalComplete( job.build )

  else:
    logging.warning( 'Job has nothing to report to, hopfully that was intentional' )

  logging.info( 'Setting job "%s" to Reported.' % job.id )
  job.reported_at = datetime.utcnow().replace( tzinfo=utc )
  job.save()

  results = ''
  resource_map = json.loads( job.resources )
  for target in resource_map:
    results += 'Target: *%s*\n' % target
    for index in range( 0, len( resource_map[ target ] ) ):
      tmp = resource_map[ target ][ index ]
      results += 'Index %s\n' % index
      try:
        results += 'Sucess: *%s*\n' % tmp[ 'success' ]
      except KeyError:
        pass
      try:
        results += '```%s```\n' % tmp[ 'results' ]
      except KeyError:
        pass

  slack.post_message( 'Job %s ( project: "%s", build: "%s", branch: "%s", target: "%s" ) Completed.\n%s' % ( job.id, job.project.name, job.build.name, job.branch, job.target, results[:500] ), slack.INFO )


# iterate over the Reported jobs that are not manual and look to see if the resources are all released
for job in BuildJob.objects.filter( built_at__isnull=False, ran_at__isnull=False, reported_at__isnull=False, acknowledged_at__isnull=True, released_at__isnull=True, manual=False ):
  if job.suceeded: # sucess auto Acknoledges
    job.acknowledged_at = datetime.utcnow().replace( tzinfo=utc )
    job.save()
    continue

  # auto acknoledge after AUTO_ACKNOLEDGE_DELAY
  if job.reported_at < ( datetime.utcnow().replace( tzinfo=utc ) - timedelta( hours=AUTO_ACKNOLEDGE_DELAY ) ):
    job.acknowledged_at = datetime.utcnow().replace( tzinfo=utc )
    job.save()
    continue


# iterate over the Acknoledged jobs, and release the resources
for job in BuildJob.objects.filter( built_at__isnull=False, ran_at__isnull=False, reported_at__isnull=False, acknowledged_at__isnull=False, released_at__isnull=True ):
  resource_map = json.loads( job.resources )
  for name in resource_map:
    for index in range( 0, len( resource_map[ name ] ) ):
      if resource_map[ name ][ index ][ 'status' ] not in ( 'Releasing', 'Released' ):
        logging.info( 'Releasing resource "%s" "%s" of job "%s".' % ( name, index, job.id ) )
        if Resource.release( resource_map[ name ][ index ][ 'config' ]  ) is None:
          job.updateResourceState( name, index, 'Released' )
        else:
          job.updateResourceState( name, index, 'Releasing' )

  resource_map = json.loads( job.resources )
  for name in resource_map:
    for index in range( 0, len( resource_map[ name ] ) ):
      if resource_map[ name ][ index ][ 'status' ] == 'Releasing':
        if Resource.released( resource_map[ name ][ index ][ 'config' ] ):
          job.updateResourceState( name, index, 'Released' )

  resource_map = json.loads( job.resources )
  ready = True
  for name in resource_map:
    for index in range( 0, len( resource_map[ name ] ) ):
      ready &= resource_map[ name ][ index ][ 'status' ] == 'Released'

  # resources all released, set job to released
  if ready:
    logging.info( 'Setting job "%s" to Released.' % job.id )
    job.released_at = datetime.utcnow().replace( tzinfo=utc )
    job.save()


# Iterate over released jobs that are at least CLEANUP_DELAY hours old and delete them
for job in BuildJob.objects.filter( released_at__lt=( datetime.utcnow().replace( tzinfo=utc ) - timedelta( hours=CLEANUP_DELAY ) ) ):
  logging.info( 'Cleaning up job "%s".' % job.id )
  for bjnr in job.buildjobnetworkresource_set.all():
    bjnr.delete()
  job.delete()

os.unlink( PID_FILE )
logging.info( 'Done!' )
logging.shutdown()
sys.exit( 0 )
